{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vgg19 import VGGUNET19\n",
    "model = VGGUNET19()\n",
    "checkpoint_path = \"VGGUnet19_Segmentation_best.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Resize(512, 512),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CustomBBoxLoss class\n",
    "class CustomBBoxLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBBoxLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def normalize_color(self, color):\n",
    "        \"\"\"\n",
    "        Normalize an RGBA color from [0, 255] to [0, 1].\n",
    "        Args:\n",
    "            color: Tuple (R, G, B, A) in [0, 255]\n",
    "        Returns:\n",
    "            Normalized tuple in [0, 1]\n",
    "        \"\"\"\n",
    "        return tuple(c / 255.0 for c in color)\n",
    "\n",
    "    def forward(self, pred, bbox_target_list):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            pred: Tensor of shape (1, C, H, W), the model's predicted output\n",
    "            bbox_target_list: List of tuples containing bbox coordinates and target color.\n",
    "                Format: [(((x1, y1), (x2, y2)), (R, G, B, A)), ...]\n",
    "\n",
    "        Returns:\n",
    "            loss: The computed loss\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "\n",
    "        for bbox, target_color in bbox_target_list:\n",
    "            # Extract bounding box coordinates\n",
    "            (x1, y1), (x2, y2) = bbox\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "\n",
    "            # Extract the predicted region within the bounding box\n",
    "            pred_region = TF.crop(pred, top=y1, left=x1, height=height, width=width)  # shape: (C, H, W)\n",
    "\n",
    "            # Normalize the target color to [0, 1]\n",
    "            target_color = self.normalize_color(target_color)\n",
    "\n",
    "            # Ensure target_tensor matches the number of channels in pred_region\n",
    "            num_channels = pred_region.shape[0]  # Get the number of channels in pred_region\n",
    "            target_tensor = torch.tensor(\n",
    "                target_color[:num_channels],  # Slice target_color to match num_channels\n",
    "                dtype=pred.dtype,\n",
    "                device=pred.device\n",
    "            ).view(num_channels, 1, 1)\n",
    "            target_tensor = target_tensor.expand_as(pred_region)\n",
    "\n",
    "            # Compute MSE loss for the current bounding box\n",
    "            loss += self.mse_loss(pred_region, target_tensor)\n",
    "\n",
    "        return loss / len(bbox_target_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PlanDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        self.img_files = [img_file for img_file in os.listdir(directory) if img_file.endswith('Plan.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        selected_img_file = self.img_files[index]\n",
    "        \n",
    "       \n",
    "        plan = Image.open(os.path.join(self.directory, selected_img_file)).convert('RGB')\n",
    "        \n",
    "        \n",
    "        plan = np.array(plan).astype(np.float32)\n",
    "        \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=plan)\n",
    "            plan = transformed['image']\n",
    "        \n",
    "        plan = torch.from_numpy(plan.copy().transpose((2, 0, 1)))  # (H, W, C) -> (C, H, W)\n",
    "        \n",
    "        return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PlanDataset(directory='train', transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box target list\n",
    "bbox_target_list = [\n",
    "    (((264, 414), (317, 448)), (254, 81, 80, 255))  # RGBA values in [0, 255]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = criterion = CustomBBoxLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = 1e-4,\n",
    "    betas = (0.9, 0.999), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_start():\n",
    "    model.train()\n",
    "    for epoch in range(1):  # Single epoch for testing\n",
    "        for idx, img_batch in enumerate(train_dataloader):\n",
    "            img_batch = img_batch.to(device)  # img_batch already has batch dimension\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(img_batch)  # Get model prediction\n",
    "\n",
    "            loss = criterion(pred, bbox_target_list)  # Compute loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Batch {idx + 1}, Loss: {loss.item()}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 0.08223015815019608\n"
     ]
    }
   ],
   "source": [
    "train_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize output\n",
    "def visualize_output(predictions, from_tensor=True):\n",
    "    color_mapping = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (255, 80, 80),\n",
    "        2: (80, 80, 255),\n",
    "        3: (255, 255, 255),\n",
    "    }\n",
    "    \n",
    "    if from_tensor:\n",
    "        predictions = torch.round(predictions).type(torch.LongTensor)\n",
    "        predictions = predictions.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    height, width = predictions.shape\n",
    "    colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for label, color in color_mapping.items():\n",
    "        colored_mask[predictions == label] = color\n",
    "\n",
    "    return Image.fromarray(colored_mask)\n",
    "\n",
    "\n",
    "# Generate function\n",
    "def generate(image, model):\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    output_image_pil = visualize_output(output)\n",
    "    \n",
    "    return output_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('6_Plan.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = generate(image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
